{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2_yg7YoVLBaY"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zWTUrWGILmVu"
   },
   "outputs": [],
   "source": [
    "train = pd.read_excel('E:/MH_text-catagorization using NLP/Data_Train.xlsx')\n",
    "test = pd.read_excel('E:/MH_text-catagorization using NLP/Data_Test.xlsx')\n",
    "submission = pd.read_excel('E:/MH_text-catagorization using NLP/Sample_submission.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "U9AVGDuvMAQ6",
    "outputId": "64e6e8e7-d376-4c9e-c889-58d4cd86a92c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "      <th>SECTION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>But the most painful was the huge reversal in ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How formidable is the opposition alliance amon...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Most Asian currencies were trading lower today...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>If you want to answer any question, click on ‘...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In global markets, gold prices edged up today ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STORY  SECTION\n",
       "0  But the most painful was the huge reversal in ...        3\n",
       "1  How formidable is the opposition alliance amon...        0\n",
       "2  Most Asian currencies were trading lower today...        3\n",
       "3  If you want to answer any question, click on ‘...        1\n",
       "4  In global markets, gold prices edged up today ...        3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "EmK7ZEWRMI99",
    "outputId": "adcfd137-f50e-408e-aa5b-0a5d54248422"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019 will see gadgets like gaming smartphones ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It has also unleashed a wave of changes in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It can be confusing to pick the right smartpho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The mobile application is integrated with a da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>We have rounded up some of the gadgets that sh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               STORY\n",
       "0  2019 will see gadgets like gaming smartphones ...\n",
       "1  It has also unleashed a wave of changes in the...\n",
       "2  It can be confusing to pick the right smartpho...\n",
       "3  The mobile application is integrated with a da...\n",
       "4  We have rounded up some of the gadgets that sh..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ey0WTw8oMdhD",
    "outputId": "02511c0e-b637-415c-ae21-037c88242cea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['STORY', 'SECTION'], dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MHWOeulTMLHE",
    "outputId": "864ebe69-9aad-4abb-8196-23c50663817c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2772\n",
       "2    1924\n",
       "0    1686\n",
       "3    1246\n",
       "Name: SECTION, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['SECTION'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0mqbz3R2O6u"
   },
   "source": [
    "#Build Train and Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "mQ8yKtHcMUpp"
   },
   "outputs": [],
   "source": [
    "# build train and test datasets\n",
    "\n",
    "train_STORY = train['STORY'].values\n",
    "train_SECTION = train['SECTION'].values\n",
    "\n",
    "test_STORY = test['STORY'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JfcitasANAW-",
    "outputId": "1d1d89c2-b2cd-4e3b-b720-a948ece65d4b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['But the most painful was the huge reversal in fee income, unheard of among private sector lenders. Essentially, it means that Yes Bank took it for granted that fees on structured loan deals will be paid and accounted for upfront on its books. As borrowers turned defaulters, the fees tied to these loan deals fell off the cracks. Gill has now vowed to shift to a safer accounting practice of amortizing fee income rather than booking these upfront.\\n\\n\\nGill’s move to mend past ways means that there will be no nasty surprises in the future. This is good news considering that investors love a clean image and loathe uncertainties.\\n\\n\\nBut there is no gain without pain and the promise of a strong and stable balance sheet comes with some sacrifices as well. Investors will have to give up the hopes of phenomenal growth, a promise made by Kapoor.',\n",
       "       'How formidable is the opposition alliance among Congress, Jharkhand Mukti Morcha (JMM) and Jharkhand Vikas Morcha (Prajatantrik)?',\n",
       "       'Most Asian currencies were trading lower today. South Korean won was down 0.4%, China renminbi 0.23%, China Offshore 0.15%, Malaysian ringgit 0.12%, Indonesian rupiah 0.11%, Taiwan dollar 0.06%. However, Japanese yen was up 0.32%.\\n\\n\\nThe dollar index, which measures the US currency’s strength against major currencies, was trading at 97.26, down 0.14% from its previous close of 97.395.',\n",
       "       ...,\n",
       "       'The database has been created after bringing together criminal records of the state police, the prisons department and the GRP that guards the railway network in the state.\\n\\n\\nA senior official involved in developing the app said the database will help policeman on ground by deploying techniques like face recognition, text search, biometric records analysis, phonetic search, artificial intelligence (AI) and gang analysis to “zero in on the criminal\" in a quick and targeted manner.',\n",
       "       'The state, which has had an uneasy relationship with the mainland since the days of the late pro-independence leader Angami Zapu Phizo, is wary of National Democratic Alliance (NDA) promises because of the Naga Peace Accord, 2015, held up by slow progress of talks and the controversial Citizenship (Amendment) Bill, 2016. These, along with the aversion of the people of the Christian-majority state toward the BJP’s agenda to promote Hindutva and the uniform civil code, are the major issues in Nagaland.\\n\\n\\nThe direct contest in Nagaland, where money power has fuelled political corruption, is between the ‘hand’ and the ‘globe’, the election symbols of the Congress and the NDPP, respectively. Many believe the BJP-NDPP alliance has an edge because the state tends to send the ruling party candidate to the Lok Sabha. “If you don’t have support in Delhi, any state project will run into roadblocks,\" said Kathi Chishi, secretary of Toka MPCS, an organization that provides financing for rural livelihood.\\n\\n\\nThe NDPP’s candidate is Tokheho Yepthomi, the current member of parliament, who won the seat last year after Neiphiu Rio vacated it to contest the assembly elections. The Congress candidate is K.L. Chishi, former chief minister and a veteran Naga politician. Conrad Sangma’s National People’s Party has fielded a candidate, while the Naga People’s Front is backing its arch rival, the Congress, because it is against the BJP’s majoritarian Hindutva agenda and believes Congress is the party with a secular plank. An independent, M.M. Thromwa Konyak, is also in the fray. The BJP released its manifesto on Monday, restating its commitment to passing the citizenship amendment bill, which had sparked violence in the North-East. However, Yepthomi said on Friday, that after “the government is formed, the NDA partners will meet and then discuss. And what will be best for all stakeholders will be decided on that basis.\"',\n",
       "       'Virus stars Kunchacko Boban, Tovino Thomas, Indrajith Sukumaran, Asif Ali, Soubin Shahir, Poornima Indrajith, Sreenath Bhasi, Rima Kallingal, Remya Nambeesan, Joju George, Dileesh Pothan, Senthil Krishna, Rahman, Revathy, Asha Kelunni, Parvathy Thiruvothu, Indrans and Madonna Sebastian'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MxyW2nWKNAZr",
    "outputId": "2b62e228-fec8-4fc7-b545-69fe77d8ee4e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 0, 3, ..., 1, 0, 2], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_SECTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6McIfMdiNAcB",
    "outputId": "b8e502be-1810-4144-e458-fc4b4d605035",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2019 will see gadgets like gaming smartphones and wearable medical devices lifting the user experience to a whole new level\\n\\n\\nmint-india-wire consumer technologyconsumer technology trends in New Yeartech gadgetsFoldable phonesgaming smartphoneswearable medical devicestechnology\\n\\n\\nNew Delhi: Gadgets have become an integral part of our lives with most of us relying on some form of factor to communicate, commute, work, be informed or entertained. Year 2019 will see some gadgets lifting the user experience to a whole new level. Here’s what we can expect to see:\\n\\n\\nSmartphones with foldable screens: Foldable phones are finally moving from the concept stage to commercial launches. They are made up of organic light-emitting diode (OLED) panels with higher plastic substrates, allowing them to be bent without damage.\\n\\n\\nUS-based display maker Royole Corp’s foldable phone, FlexPai, has already arrived in select markets, while Samsung’s unnamed foldable phone is expected sometime next year. Samsung’s smartphone chief executive officer D.J. Koh has said they will make a million units of it. LG, too, is expected to display a foldable phone next year. Meanwhile Apple, Nokia, Lenovo and Huawei have also been working on foldable phones, reportedly.\\n\\n\\neSIM: Very soon your smartphone won’t need a physical SIM card anymore. The eSIM technology, already used by Apple in its iPhones and Apple Watch, replaces the physical SIM with a virtually embedded chip on the motherboard. eSIMs support multiple mobile operators and can be programmed to switch services.',\n",
       "       'It has also unleashed a wave of changes in the MCU that will make sure its future is a lot different than its past\\n\\n\\nKevin Feige had signalled diversity and more representation in the post-phase 3 MCU and Endgame does a lot to showcase the initiative',\n",
       "       'It can be confusing to pick the right smartphone for yourself, so we have segregated the top smartphones under Rs 20,000 according to their strengths.\\n\\n\\nThe best smartphones under ₹20,000 categorised according to performance, camera, design and battery life\\n\\n\\nmint-india-wire phones under Rs 20000Poco F1Realme U1Redmi Note 6 Prorealme 2 proHonor PlayNokia 7.1Nova 3iAsus Zenfone Max Pro M1\\n\\n\\nGone are the days when you had to shell out big buck for buying smartphones with premium features. Technology has become more accessible recently and the biggest example of that lies in the sub-Rs 20,000 category—you get good performance, design and even software at a reasonable price.\\n\\n\\nIt can be confusing to pick the right smartphone for you, however, given the amount of variety that lies in the segment. So we have segregated the top smartphones under ₹ 20,000 according to their strengths, so you can pick the one that suits you best.\\n\\n\\nThis phone actually lies just north of the ₹ 20,000 price point. But if you have an HDFC debit or credit card, you can purchase the lowest spec variant with 6GB RAM and 64GB internal storage for as low as ₹ 19,999, making it the cheapest smartphone to run a Qualcomm Snapdragon 845 SoC. There’s not a lot to not like about this phone—it has the fastest processor Qualcomm has to offer, some thermal trickery to keep your smartphone cool during intense gaming sessions, a very good camera and some durable plastic that doesn’t shatter or pick up scratches.\\n\\n\\nIt even gets a modded version of the MIUI with an app drawer that allows you colour code your applications.',\n",
       "       ...,\n",
       "       'On the photography front, the Note 5 Pro features a 12MP + 15MP dual rear camera setup, sporting an aperture of f/2.2, while on the front it has a 20MP selfie camera.\\n\\n\\nXiaomi Redmi Note 5 ProRedmi Note 5 Pro price cutRedmi Note 5 Pro price indiaRedmi Note 5 Pro specificationsRedmi Note 5 Pro saleRedmi Note 5 Pro units soldRedmi Note 5 Pro review',\n",
       "       'UDAY mandated that discoms bring the gap between average revenue and average costs to zero. Here, too, there has been progress with the gap reducing (from ₹0.60/kWh in 2015-16 to ₹0.17/kWh in 2017-18), but 21 of 26 states are still unlikely to meet the target.\\n\\n\\nAccording to a National Institute of Public Finance and Policy study, this data suggests that the UDAY scheme is failing to turn around the power sector. The authors also find that discoms remain plagued by operational inefficiencies such as lack of effective billing procedures, poor measurement of power consumption, and ineffective monitoring of power theft.\\n\\n\\nTaken together, data shows that the NDA has built on the previous government’s work to get more households electrified—but quality of access still remains an issue.\\n\\n\\nWhile the government has tried to address this by improving the financial health of discoms, much more needs to be done. The inability of successive governments to revive discom fortunes could have important ramifications for India’s development.\\n\\n\\nThe World Bank estimates India’s electricity demand to treble by 2040. Addressing this rising demand will be critical for the development agenda of whichever party is elected to form the next government.',\n",
       "       'Ripple also helps bank customers send money to people in many emerging markets including Mexico, India, and Thailand to increase their share of “this large and growing market\". What’s next? “Ripple is moving beyond blockchain, and connecting networks so that we can move money across networks. Again this is open-source and lightweight so it becomes easy to transfer money across networks. So we are building the ecosystem for networks to connect with each other and in our view globalization will be completed when data, goods and money flow seamlessly. That’s the way we think of it as an internet of value when the whole world gets connected through payment systems,\" Gupta said.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_STORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_section = submission['SECTION']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQqkWQ-4NAeU"
   },
   "source": [
    "## Text Wrangling & Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##conda install pyahocorasick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MVLoIvV54UF8",
    "outputId": "e423f7f6-20ee-4089-e2bf-75b1b84eb408"
   },
   "outputs": [],
   "source": [
    "##!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "import tqdm\n",
    "import unicodedata\n",
    "\n",
    "\n",
    "def strip_html_tags(text):\n",
    "  soup = BeautifulSoup(text, \"html.parser\")\n",
    "  [s.extract() for s in soup(['iframe', 'script'])]\n",
    "  stripped_text = soup.get_text()\n",
    "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
    "  return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "  return text\n",
    "\n",
    "def pre_process_corpus(docs):\n",
    "  norm_docs = []\n",
    "  for doc in tqdm.tqdm(docs):\n",
    "    doc = strip_html_tags(doc)\n",
    "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
    "    doc = doc.lower()\n",
    "    doc = remove_accented_chars(doc)\n",
    "    doc = contractions.fix(doc)\n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    doc = doc.strip()  \n",
    "    norm_docs.append(doc)\n",
    "  \n",
    "  return norm_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aZOoV8a6NAin",
    "outputId": "7f51f484-f6db-47e2-ac1a-3fc5fb27adcd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7628/7628 [00:03<00:00, 2269.00it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 2748/2748 [00:01<00:00, 1645.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.04 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "norm_train_story = pre_process_corpus(train_STORY)\n",
    "norm_test_story = pre_process_corpus(test_STORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M56Eedc24yVB"
   },
   "source": [
    "\n",
    "# Traditional Supervised Machine Learning Models\n",
    "## feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XyGwUMRCNAnM",
    "outputId": "98b78ebd-fa2b-4bb0-9aa9-46bf9ce84ad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# build BOW features on train reviews\n",
    "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
    "cv_train_features = cv.fit_transform(norm_train_story)\n",
    "\n",
    "\n",
    "# build TFIDF features on train reviews\n",
    "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
    "                     sublinear_tf=True)\n",
    "tv_train_features = tv.fit_transform(norm_train_story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ALjdkeEANAo4",
    "outputId": "8f8a2eb8-a18a-463f-81b9-5bbf247d026f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# transform test reviews into features\n",
    "cv_test_features = cv.transform(norm_test_story)\n",
    "tv_test_features = tv.transform(norm_test_story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C4RqD75qNAqh",
    "outputId": "cacc0b2a-4019-4831-f715-c36544933093"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW model:> Train features shape: (7628, 31941)  Test features shape: (2748, 31941)\n",
      "TFIDF model:> Train features shape: (7628, 31941)  Test features shape: (2748, 31941)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('BOW model:> Train features shape:', cv_train_features.shape, ' Test features shape:', cv_test_features.shape)\n",
    "print('TFIDF model:> Train features shape:', tv_train_features.shape, ' Test features shape:', tv_test_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvUCVm1y5bBI"
   },
   "source": [
    "\n",
    "# Model Training, Prediction and Performance Evaluation\n",
    "## Try out Logistic Regression\n",
    "The logistic regression model is actually a statistical model developed by statistician David Cox in 1958. It is also known as the logit or logistic model since it uses the logistic (popularly also known as sigmoid) mathematical function to estimate the parameter values. These are the coefficients of all our features such that the overall loss is minimized when predicting the outcome—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_nET-a59NAxd",
    "outputId": "2a6f895a-05c5-4460-ebd7-beffbbbea376"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Logistic Regression model on BOW features\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# instantiate model\n",
    "lr = LogisticRegression(penalty='l2', max_iter=500, C=1, solver='lbfgs', random_state=42)\n",
    "\n",
    "# train model\n",
    "lr.fit(cv_train_features, train_SECTION)\n",
    "\n",
    "# predict on test data\n",
    "lr_bow_predictions = lr.predict(cv_test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbfMZrCl6Qyn",
    "outputId": "ef6978b7-6058-4c90-d38c-fe7c74ebc8fc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, ..., 1, 3, 1], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_bow_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download from colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "V_Cnd05573U0"
   },
   "outputs": [],
   "source": [
    "df_lr = pd.DataFrame (lr_bow_predictions)\n",
    "submission['SECTION'] = df_lr.values\n",
    "filepath = 'MH_news-text_catagorzation_LR.xlsx'\n",
    "submission.to_excel(filepath, index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "2JpfJD9Q-caS",
    "outputId": "67b758f7-08e3-4129-c741-63258534f6c4"
   },
   "outputs": [],
   "source": [
    "#from google.colab import files\n",
    "#files.download('MH_news-text_catagorzation_LR.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x63-78AiNBCf"
   },
   "source": [
    "# Newer Supervised Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "Rty8VFR1NBGe"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout, Activation, Dense\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Politics', 'Technology', 'Entertainment', 'Business']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqNGGVlxNBI0"
   },
   "source": [
    "# Prediction class label encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "-h6SgkrrNBLr",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "# tokenize train reviews & encode train labels\n",
    "tokenized_train = [nltk.word_tokenize(text)\n",
    "                       for text in norm_train_story]\n",
    "y_train = le.fit_transform(train_SECTION)\n",
    "# tokenize test reviews & encode test labels\n",
    "tokenized_test = [nltk.word_tokenize(text)\n",
    "                       for text in norm_test_story]\n",
    "y_test = le.fit_transform(sub_section)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "qcpIQZO2NBPG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "section class label map: {0: 0, 1: 1, 2: 2, 3: 3}\n",
      "Sample test label transformation:\n",
      "----------------------------------- \n",
      "Actual Labels: 0       1\n",
      "1       2\n",
      "2       1\n",
      "3       0\n",
      "4       1\n",
      "       ..\n",
      "2743    1\n",
      "2744    1\n",
      "2745    1\n",
      "2746    3\n",
      "2747    1\n",
      "Name: SECTION, Length: 2748, dtype: int64 \n",
      "Encoded Labels: [1 2 1]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print class label encoding map and encoded labels\n",
    "print('section class label map:', dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "print('Sample test label transformation:\\n'+'-'*35,\n",
    "      '\\nActual Labels:', sub_section, '\\nEncoded Labels:', y_test[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "Ca04y7RyNBRh"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-16 23:27:22,284 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2021-04-16 23:27:22,286 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2021-04-16 23:27:22,290 : INFO : Dictionary lifecycle event {'msg': \"built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\", 'datetime': '2021-04-16T23:27:22.290408', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "gARfvctWNBUJ",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-04-16 23:38:28,311 : INFO : collecting all words and their counts\n",
      "2021-04-16 23:38:28,313 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-04-16 23:38:28,678 : INFO : collected 40121 word types from a corpus of 821529 raw words and 7628 sentences\n",
      "2021-04-16 23:38:28,680 : INFO : Creating a fresh vocabulary\n",
      "2021-04-16 23:38:28,757 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 retains 6846 unique words (17.063383265621496%% of original 40121, drops 33275)', 'datetime': '2021-04-16T23:38:28.757422', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-04-16 23:38:28,759 : INFO : Word2Vec lifecycle event {'msg': 'effective_min_count=10 leaves 746491 word corpus (90.86605585438859%% of original 821529, drops 75038)', 'datetime': '2021-04-16T23:38:28.759417', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-04-16 23:38:28,869 : INFO : deleting the raw counts dictionary of 40121 items\n",
      "2021-04-16 23:38:28,872 : INFO : sample=0.001 downsamples 36 most-common words\n",
      "2021-04-16 23:38:28,873 : INFO : Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 572095.2872328092 word corpus (76.6%% of prior 746491)', 'datetime': '2021-04-16T23:38:28.873200', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "2021-04-16 23:38:29,027 : INFO : estimated required memory for 6846 words and 100 dimensions: 8899800 bytes\n",
      "2021-04-16 23:38:29,029 : INFO : resetting layer weights\n",
      "2021-04-16 23:38:29,039 : INFO : Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2021-04-16T23:38:29.039357', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "2021-04-16 23:38:29,040 : INFO : Word2Vec lifecycle event {'msg': 'training model with 4 workers on 6846 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=150', 'datetime': '2021-04-16T23:38:29.040361', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-04-16 23:38:30,097 : INFO : EPOCH 1 - PROGRESS: at 35.49% examples, 190964 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:31,124 : INFO : EPOCH 1 - PROGRESS: at 73.83% examples, 203048 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:31,653 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-04-16 23:38:31,728 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-16 23:38:31,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-16 23:38:31,750 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-16 23:38:31,752 : INFO : EPOCH - 1 : training on 821529 raw words (572260 effective words) took 2.7s, 211663 effective words/s\n",
      "2021-04-16 23:38:32,761 : INFO : EPOCH 2 - PROGRESS: at 36.59% examples, 206871 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:33,766 : INFO : EPOCH 2 - PROGRESS: at 69.06% examples, 196139 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:34,470 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-04-16 23:38:34,527 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-16 23:38:34,548 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-16 23:38:34,564 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-16 23:38:34,566 : INFO : EPOCH - 2 : training on 821529 raw words (572222 effective words) took 2.8s, 203888 effective words/s\n",
      "2021-04-16 23:38:35,607 : INFO : EPOCH 3 - PROGRESS: at 35.49% examples, 193780 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:36,664 : INFO : EPOCH 3 - PROGRESS: at 73.83% examples, 201599 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:37,258 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-04-16 23:38:37,316 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-16 23:38:37,342 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-16 23:38:37,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-16 23:38:37,349 : INFO : EPOCH - 3 : training on 821529 raw words (572240 effective words) took 2.8s, 206174 effective words/s\n",
      "2021-04-16 23:38:38,367 : INFO : EPOCH 4 - PROGRESS: at 35.49% examples, 198346 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:39,380 : INFO : EPOCH 4 - PROGRESS: at 70.11% examples, 198055 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:40,134 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-04-16 23:38:40,165 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-16 23:38:40,200 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-16 23:38:40,226 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-16 23:38:40,227 : INFO : EPOCH - 4 : training on 821529 raw words (572579 effective words) took 2.9s, 199455 effective words/s\n",
      "2021-04-16 23:38:41,302 : INFO : EPOCH 5 - PROGRESS: at 30.99% examples, 161432 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:42,314 : INFO : EPOCH 5 - PROGRESS: at 68.89% examples, 189241 words/s, in_qsize 7, out_qsize 0\n",
      "2021-04-16 23:38:43,011 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-04-16 23:38:43,067 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-04-16 23:38:43,076 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-04-16 23:38:43,096 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-04-16 23:38:43,097 : INFO : EPOCH - 5 : training on 821529 raw words (572403 effective words) took 2.9s, 199916 effective words/s\n",
      "2021-04-16 23:38:43,099 : INFO : Word2Vec lifecycle event {'msg': 'training on 4107645 raw words (2861704 effective words) took 14.1s, 203573 effective words/s', 'datetime': '2021-04-16T23:38:43.099533', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "2021-04-16 23:38:43,100 : INFO : Word2Vec lifecycle event {'params': 'Word2Vec(vocab=6846, vector_size=100, alpha=0.025)', 'datetime': '2021-04-16T23:38:43.100331', 'gensim': '4.0.1', 'python': '3.8.5 (default, Sep  3 2020, 21:29:08) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.8 s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%time\n",
    "# build word2vec model\n",
    "w2v_model = gensim.models.Word2Vec(tokenized_train, window=150,\n",
    "                                   min_count=10, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "SE21arQ1NBZG"
   },
   "outputs": [],
   "source": [
    "def averaged_word2vec_vectorizer(corpus, model, num_features):\n",
    "    vocabulary = set(model.wv.index_to_key)\n",
    "    \n",
    "    def average_word_vectors(words, model, vocabulary, num_features):\n",
    "        feature_vector = np.zeros((num_features,), dtype=\"float64\")\n",
    "        nwords = 0.\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary: \n",
    "                nwords = nwords + 1.\n",
    "                feature_vector = np.add(feature_vector, model.wv[word])\n",
    "        if nwords:\n",
    "            feature_vector = np.divide(feature_vector, nwords)\n",
    "\n",
    "        return feature_vector\n",
    "\n",
    "    features = [average_word_vectors(tokenized_sentence, model, vocabulary, num_features)\n",
    "                    for tokenized_sentence in corpus]\n",
    "    return np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "-oEnd9VZNBcx"
   },
   "outputs": [],
   "source": [
    "w2v_num_features = 100\n",
    "# generate averaged word vector features from word2vec model\n",
    "avg_wv_train_features = averaged_word2vec_vectorizer(corpus=tokenized_train, model=w2v_model,\n",
    "                                                     num_features=w2v_num_features)\n",
    "avg_wv_test_features = averaged_word2vec_vectorizer(corpus=tokenized_test, model=w2v_model,\n",
    "                                                    num_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec model:> Train features shape: (7628, 100)  Test features shape: (2748, 100)\n"
     ]
    }
   ],
   "source": [
    "print('Word2Vec model:> Train features shape:', avg_wv_train_features.shape, ' Test features shape:', avg_wv_test_features.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling with deep neural networks\n",
    "## Building Deep neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_deepnn_architecture(num_input_features):\n",
    "    dnn_model = Sequential()\n",
    "    dnn_model.add(Dense(512, input_shape=(num_input_features,), kernel_initializer='he_normal'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('elu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('elu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(256, kernel_initializer='he_normal'))\n",
    "    dnn_model.add(BatchNormalization())\n",
    "    dnn_model.add(Activation('elu'))\n",
    "    dnn_model.add(Dropout(0.2))\n",
    "    \n",
    "    dnn_model.add(Dense(4))\n",
    "    dnn_model.add(Activation('softmax'))\n",
    "\n",
    "    dnn_model.compile(loss='categorical_crossentropy', optimizer='adam',                 \n",
    "                      metrics=['accuracy'])\n",
    "    return dnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_dnn = construct_deepnn_architecture(num_input_features=w2v_num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 1028      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 253,956\n",
      "Trainable params: 251,908\n",
      "Non-trainable params: 2,048\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "w2v_dnn.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training, Prediction and Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import keras.utils\n",
    "from keras import utils as np_utils\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       ...,\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_categorical(train['SECTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "69/69 [==============================] - 4s 19ms/step - loss: 0.3642 - accuracy: 0.8706 - val_loss: 0.1771 - val_accuracy: 0.9423\n",
      "Epoch 2/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1717 - accuracy: 0.9471 - val_loss: 0.1479 - val_accuracy: 0.9502\n",
      "Epoch 3/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1672 - accuracy: 0.9451 - val_loss: 0.1484 - val_accuracy: 0.9489\n",
      "Epoch 4/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1696 - accuracy: 0.9468 - val_loss: 0.1411 - val_accuracy: 0.9581\n",
      "Epoch 5/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1414 - accuracy: 0.9507 - val_loss: 0.1601 - val_accuracy: 0.9463\n",
      "Epoch 6/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1502 - accuracy: 0.9468 - val_loss: 0.1478 - val_accuracy: 0.9541\n",
      "Epoch 7/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1428 - accuracy: 0.9544 - val_loss: 0.1377 - val_accuracy: 0.9554\n",
      "Epoch 8/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1471 - accuracy: 0.9483 - val_loss: 0.1339 - val_accuracy: 0.9528\n",
      "Epoch 9/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1429 - accuracy: 0.9504 - val_loss: 0.1388 - val_accuracy: 0.9567\n",
      "Epoch 10/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1350 - accuracy: 0.9554 - val_loss: 0.1555 - val_accuracy: 0.9476\n",
      "Epoch 11/50\n",
      "69/69 [==============================] - 1s 16ms/step - loss: 0.1350 - accuracy: 0.9535 - val_loss: 0.1271 - val_accuracy: 0.9554\n",
      "Epoch 12/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1305 - accuracy: 0.9560 - val_loss: 0.1371 - val_accuracy: 0.9581\n",
      "Epoch 13/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1391 - accuracy: 0.9502 - val_loss: 0.1333 - val_accuracy: 0.9567\n",
      "Epoch 14/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 0.1283 - accuracy: 0.9562 - val_loss: 0.1354 - val_accuracy: 0.9567\n",
      "Epoch 15/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1256 - accuracy: 0.9558 - val_loss: 0.1385 - val_accuracy: 0.9554\n",
      "Epoch 16/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1372 - accuracy: 0.9540 - val_loss: 0.1357 - val_accuracy: 0.9554\n",
      "Epoch 17/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1290 - accuracy: 0.9537 - val_loss: 0.1361 - val_accuracy: 0.9502\n",
      "Epoch 18/50\n",
      "69/69 [==============================] - 1s 16ms/step - loss: 0.1117 - accuracy: 0.9608 - val_loss: 0.1498 - val_accuracy: 0.9528\n",
      "Epoch 19/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1308 - accuracy: 0.9566 - val_loss: 0.1381 - val_accuracy: 0.9515\n",
      "Epoch 20/50\n",
      "69/69 [==============================] - 1s 19ms/step - loss: 0.1262 - accuracy: 0.9575 - val_loss: 0.1300 - val_accuracy: 0.9541\n",
      "Epoch 21/50\n",
      "69/69 [==============================] - 1s 15ms/step - loss: 0.1188 - accuracy: 0.9572 - val_loss: 0.1296 - val_accuracy: 0.9502\n",
      "Epoch 22/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1299 - accuracy: 0.9533 - val_loss: 0.1267 - val_accuracy: 0.9567\n",
      "Epoch 23/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1310 - accuracy: 0.9548 - val_loss: 0.1431 - val_accuracy: 0.9541\n",
      "Epoch 24/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1292 - accuracy: 0.9575 - val_loss: 0.1501 - val_accuracy: 0.9528\n",
      "Epoch 25/50\n",
      "69/69 [==============================] - 1s 15ms/step - loss: 0.1226 - accuracy: 0.9565 - val_loss: 0.1364 - val_accuracy: 0.9581\n",
      "Epoch 26/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1253 - accuracy: 0.9558 - val_loss: 0.1306 - val_accuracy: 0.9567\n",
      "Epoch 27/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1139 - accuracy: 0.9626 - val_loss: 0.1260 - val_accuracy: 0.9567\n",
      "Epoch 28/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1207 - accuracy: 0.9589 - val_loss: 0.1510 - val_accuracy: 0.9502\n",
      "Epoch 29/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1131 - accuracy: 0.9605 - val_loss: 0.1364 - val_accuracy: 0.9541\n",
      "Epoch 30/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1083 - accuracy: 0.9568 - val_loss: 0.1320 - val_accuracy: 0.9515\n",
      "Epoch 31/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1089 - accuracy: 0.9603 - val_loss: 0.1420 - val_accuracy: 0.9528\n",
      "Epoch 32/50\n",
      "69/69 [==============================] - 1s 18ms/step - loss: 0.1133 - accuracy: 0.9615 - val_loss: 0.1363 - val_accuracy: 0.9515\n",
      "Epoch 33/50\n",
      "69/69 [==============================] - 1s 17ms/step - loss: 0.1183 - accuracy: 0.9607 - val_loss: 0.1333 - val_accuracy: 0.9528\n",
      "Epoch 34/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1011 - accuracy: 0.9650 - val_loss: 0.1302 - val_accuracy: 0.9515\n",
      "Epoch 35/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1007 - accuracy: 0.9668 - val_loss: 0.1311 - val_accuracy: 0.9528\n",
      "Epoch 36/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.0994 - accuracy: 0.9648 - val_loss: 0.1405 - val_accuracy: 0.9502\n",
      "Epoch 37/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1120 - accuracy: 0.9627 - val_loss: 0.1271 - val_accuracy: 0.9581\n",
      "Epoch 38/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1083 - accuracy: 0.9619 - val_loss: 0.1235 - val_accuracy: 0.9607\n",
      "Epoch 39/50\n",
      "69/69 [==============================] - 1s 16ms/step - loss: 0.1063 - accuracy: 0.9610 - val_loss: 0.1205 - val_accuracy: 0.9515\n",
      "Epoch 40/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1070 - accuracy: 0.9639 - val_loss: 0.1306 - val_accuracy: 0.9515\n",
      "Epoch 41/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1046 - accuracy: 0.9619 - val_loss: 0.1349 - val_accuracy: 0.9502\n",
      "Epoch 42/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1161 - accuracy: 0.9598 - val_loss: 0.1162 - val_accuracy: 0.9528\n",
      "Epoch 43/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.1074 - accuracy: 0.9606 - val_loss: 0.1360 - val_accuracy: 0.9581\n",
      "Epoch 44/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.0998 - accuracy: 0.9622 - val_loss: 0.1356 - val_accuracy: 0.9554\n",
      "Epoch 45/50\n",
      "69/69 [==============================] - 1s 14ms/step - loss: 0.1065 - accuracy: 0.9613 - val_loss: 0.1422 - val_accuracy: 0.9528\n",
      "Epoch 46/50\n",
      "69/69 [==============================] - 1s 15ms/step - loss: 0.1013 - accuracy: 0.9662 - val_loss: 0.1328 - val_accuracy: 0.9528\n",
      "Epoch 47/50\n",
      "69/69 [==============================] - 1s 13ms/step - loss: 0.0939 - accuracy: 0.9617 - val_loss: 0.1277 - val_accuracy: 0.9581\n",
      "Epoch 48/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.0968 - accuracy: 0.9644 - val_loss: 0.1238 - val_accuracy: 0.9581\n",
      "Epoch 49/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.0988 - accuracy: 0.9634 - val_loss: 0.1385 - val_accuracy: 0.9515\n",
      "Epoch 50/50\n",
      "69/69 [==============================] - 1s 12ms/step - loss: 0.1154 - accuracy: 0.9600 - val_loss: 0.1304 - val_accuracy: 0.9528\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x159efe57d90>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "w2v_dnn.fit(avg_wv_train_features, to_categorical(train['SECTION']), epochs=50, batch_size=batch_size, \n",
    "            shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = w2v_dnn.predict_classes(avg_wv_test_features)\n",
    "predictions = le.inverse_transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dnn = pd.DataFrame (predictions)\n",
    "submission['SECTION'] = df_dnn.values\n",
    "filepath = 'MH_news-text_catagorzation_DNN.xlsx'\n",
    "submission.to_excel(filepath, index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "t = tf.keras.preprocessing.text.Tokenizer(oov_token='<UNK>')\n",
    "# fit the tokenizer on the documents\n",
    "t.fit_on_texts(norm_train_story)\n",
    "t.word_index['<PAD>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(t.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = t.texts_to_sequences(norm_train_story)\n",
    "test_sequences = t.texts_to_sequences(norm_test_story)\n",
    "X_train = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, maxlen=1000)\n",
    "X_test = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, maxlen=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 1000, 300)         12036900  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d (SpatialDr (None, 1000, 300)         0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 4)                 1028      \n",
      "=================================================================\n",
      "Total params: 12,290,600\n",
      "Trainable params: 12,290,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300 # dimension for dense embeddings for each token\n",
    "LSTM_DIM = 128 # total LSTM units\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM, input_length=1000))\n",
    "model.add(tf.keras.layers.SpatialDropout1D(0.1))\n",
    "model.add(tf.keras.layers.LSTM(LSTM_DIM, return_sequences=False))\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(4, activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "69/69 [==============================] - 458s 7s/step - loss: 1.1325 - accuracy: 0.4802 - val_loss: 0.2716 - val_accuracy: 0.9135\n",
      "Epoch 2/10\n",
      "69/69 [==============================] - 471s 7s/step - loss: 0.1381 - accuracy: 0.9572 - val_loss: 0.1630 - val_accuracy: 0.9463\n",
      "Epoch 3/10\n",
      "69/69 [==============================] - 415s 6s/step - loss: 0.0463 - accuracy: 0.9874 - val_loss: 0.1441 - val_accuracy: 0.9567\n",
      "Epoch 4/10\n",
      "69/69 [==============================] - 412s 6s/step - loss: 0.0175 - accuracy: 0.9940 - val_loss: 0.1357 - val_accuracy: 0.9554\n",
      "Epoch 5/10\n",
      "69/69 [==============================] - 414s 6s/step - loss: 0.0091 - accuracy: 0.9972 - val_loss: 0.1260 - val_accuracy: 0.9646\n",
      "Epoch 6/10\n",
      "69/69 [==============================] - 414s 6s/step - loss: 0.0107 - accuracy: 0.9962 - val_loss: 0.1534 - val_accuracy: 0.9581\n",
      "Epoch 7/10\n",
      "69/69 [==============================] - 407s 6s/step - loss: 0.0075 - accuracy: 0.9962 - val_loss: 0.1379 - val_accuracy: 0.9633\n",
      "Epoch 8/10\n",
      "69/69 [==============================] - 409s 6s/step - loss: 0.0073 - accuracy: 0.9968 - val_loss: 0.1366 - val_accuracy: 0.9646\n",
      "Epoch 9/10\n",
      "69/69 [==============================] - 413s 6s/step - loss: 0.0068 - accuracy: 0.9969 - val_loss: 0.1584 - val_accuracy: 0.9607\n",
      "Epoch 10/10\n",
      "69/69 [==============================] - 415s 6s/step - loss: 0.0106 - accuracy: 0.9957 - val_loss: 0.1714 - val_accuracy: 0.9633\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x159f5c24970>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "model.fit(X_train, to_categorical(train['SECTION']), epochs=10, batch_size=batch_size, \n",
    "          shuffle=True, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 1, 1, 1, 2, 1, 2, 0, 3, 2, 1, 2, 1, 3, 2, 3, 2],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_lstm = model.predict_classes(X_test)\n",
    "pred_lstm[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lstm = pd.DataFrame (pred_lstm)\n",
    "submission['SECTION'] = df_lstm.values\n",
    "filepath = 'MH_news-text_catagorzation_lstm.xlsx'\n",
    "submission.to_excel(filepath, index= False)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MH_news-text_catagorzation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
